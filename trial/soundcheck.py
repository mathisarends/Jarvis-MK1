import json
import queue
import os
from dotenv import load_dotenv

import openai
import sounddevice as sd
from vosk import Model, KaldiRecognizer

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("âŒ Kein OpenAI API-Key gefunden! Bitte in .env speichern.")

openai.api_key = OPENAI_API_KEY

# ğŸ¤ Vosk Modell fÃ¼r Wake-Word-Erkennung
MODEL_PATH = "../vosk-model-small-de-0.15"
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"âŒ Vosk-Modell nicht gefunden: {MODEL_PATH}")

model = Model(MODEL_PATH)
recognizer = KaldiRecognizer(model, 16000)

audio_queue = queue.Queue()

WAKE_WORD = "jarvis"

# ğŸ”Š Callback-Funktion fÃ¼r die Mikrofonaufnahme
def callback(indata, frames, time, status):
    if status:
        print(f"âš ï¸ Sounddevice-Fehler: {status}", flush=True)
    audio_queue.put(bytes(indata))

# ğŸ“¢ Whisper-Transkription Ã¼ber OpenAI API
def transcribe_with_whisper(audio_bytes):
    print("â³ Sende Audio an Whisper...")
    
    response = openai.Audio.transcriptions.create("whisper-1", audio_bytes, filetype="wav")
    text = response.get("text", "").strip()

    if text:
        print(f"ğŸ¤ Whisper-Transkription: {text}")
    else:
        print("âš ï¸ Keine Sprache erkannt.")

# ğŸ™ï¸ Mikrofonaufnahme starten
with sd.RawInputStream(samplerate=16000, blocksize=4000, dtype='int16',
                       channels=1, callback=callback):
    print("ğŸ¤ Sprich jetzt ... Sage das Wake-Word zum Aktivieren.")

    recording = False
    audio_data = []

    while True:
        data = audio_queue.get()
        
        # ğŸ¯ Falls kein Signal erkannt wird â†’ Nichts tun
        if not recognizer.AcceptWaveform(data):
            continue
        
        # ğŸ“ Erkannten Text aus Vosk abrufen
        result = json.loads(recognizer.Result())
        text = result.get("text", "").lower()

        print(f"ğŸ” Erkannt: {text}")

        # ğŸ¯ Wake-Word erkannt â†’ Aufnahme starten
        if WAKE_WORD in text:
            print("ğŸš€ Wake-Word erkannt! Aufnahme gestartet...")
            recording = True
            audio_data.clear()
            continue

        # ğŸ¯ Falls aktuell keine Aufnahme lÃ¤uft â†’ Ãœberspringen
        if not recording:
            continue

        # ğŸ™ï¸ WÃ¤hrend der Aufnahme â†’ Audio speichern
        audio_data.append(data)

        # ğŸ¯ Aufnahme beenden, wenn "stop" erkannt wird oder genug Audio gesammelt wurde
        if "stop" in text or len(audio_data) > 50:
            print("â¹ï¸ Aufnahme beendet. Senden an Whisper...")
            recording = False

            # ğŸ“¢ Whisper API aufrufen (direkt aus dem Speicher, kein Speichern notwendig)
            transcribe_with_whisper(b"".join(audio_data))
